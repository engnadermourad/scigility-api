{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d06a3a0-dbce-48de-8f69-d5839885a673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+-------+----------+------+---------+\n",
      "|trans_id|account_id|amount|balance|      date|  type|operation|\n",
      "+--------+----------+------+-------+----------+------+---------+\n",
      "|  971490|      1645|  43.0|19460.0|1996-11-30|PRIJEM|     NULL|\n",
      "|  589995|      3273|  15.0|26679.0|1997-04-30| VYDAJ|    VYBER|\n",
      "+--------+----------+------+-------+----------+------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|account_id|district_id|\n",
      "+----------+-----------+\n",
      "|         1|         18|\n",
      "|         2|          1|\n",
      "+----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in 'trans_df': 1056410\n",
      "Number of rows in 'account_df': 4500\n",
      "Sample valid transactions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+-------+----------+------+--------------+-----------+\n",
      "|account_id|trans_id|amount|balance|      date|  type|     operation|district_id|\n",
      "+----------+--------+------+-------+----------+------+--------------+-----------+\n",
      "|      1645|  971490|  43.0|19460.0|1996-11-30|PRIJEM|          NULL|         50|\n",
      "|      3273|  589995|  15.0|26679.0|1997-04-30| VYDAJ|         VYBER|         74|\n",
      "|      1580|  285772|8400.0|41404.0|1998-04-12| VYDAJ|         VYBER|         69|\n",
      "|      2366|  425228| 388.0|24110.0|1998-02-07| VYDAJ|PREVOD NA UCET|         68|\n",
      "|      4519|  745408|2100.0|26200.0|1994-12-05|PRIJEM|         VKLAD|          8|\n",
      "+----------+--------+------+-------+----------+------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in 'valid_trans_df': 1056320\n",
      "+----------+--------+------+-------+----------+------+---------+-----------+\n",
      "|account_id|trans_id|amount|balance|      date|  type|operation|district_id|\n",
      "+----------+--------+------+-------+----------+------+---------+-----------+\n",
      "|      1645|  971490|  43.0|19460.0|1996-11-30|PRIJEM|     NULL|         50|\n",
      "|      3273|  589995|  15.0|26679.0|1997-04-30| VYDAJ|    VYBER|         74|\n",
      "+----------+--------+------+-------+----------+------+---------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/31 21:35:07 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions with invalid account_id: 90\n",
      "+----------+--------+------+-------+----------+-----+---------+\n",
      "|account_id|trans_id|amount|balance|      date| type|operation|\n",
      "+----------+--------+------+-------+----------+-----+---------+\n",
      "|     11500|  580601|4186.0|30123.0|1998-05-09|VYDAJ|    VYBER|\n",
      "|     11639|  858463|7753.0|22856.0|1994-11-07|VYDAJ|    VYBER|\n",
      "+----------+--------+------+-------+----------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, FloatType\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.functions import col, broadcast\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get AWS credentials from environment variables\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "# Check if AWS credentials are present\n",
    "if not aws_access_key_id or not aws_secret_access_key:\n",
    "    raise ValueError(\"AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are required.\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Filter Invalid Transctions\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key_id) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .config(\"spark.master\", \"spark://spark-master-2:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"62\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the 'trans' and 'account' tables\n",
    "trans_schema = StructType([\n",
    "    StructField(\"trans_id\", IntegerType(), True),\n",
    "    StructField(\"account_id\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"operation\", StringType(), True),\n",
    "    StructField(\"amount\", FloatType(), True),\n",
    "    StructField(\"balance\", FloatType(), True),\n",
    "    StructField(\"k_symbol\", StringType(), True),\n",
    "    StructField(\"branch\", StringType(), True),\n",
    "    StructField(\"bank\", StringType(), True),\n",
    "    StructField(\"account\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "account_schema = StructType([\n",
    "    StructField(\"account_id\", IntegerType(), True),\n",
    "    StructField(\"district_id\", IntegerType(), True),\n",
    "    StructField(\"frequency\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read the 'trans' and 'account' CSV files select for just necessary coloumns \n",
    "\n",
    "trans_df = spark.read \\\n",
    "    .option(\"delimiter\", \";\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(trans_schema) \\\n",
    "    .csv(\"s3a://nmourmx-scigility/Bronze/trans/trans.csv\") \\\n",
    "    .select(\"trans_id\", \"account_id\", \"amount\", \"balance\", \"date\", \"type\", \"operation\")\n",
    "\n",
    "account_df = spark.read \\\n",
    "    .option(\"delimiter\", \";\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(account_schema) \\\n",
    "    .csv(\"s3a://nmourmx-scigility/Bronze/account/account.csv\") \\\n",
    "    .select(\"account_id\", \"district_id\")\n",
    "\n",
    "\n",
    "# Ensure both use the same data type\n",
    "### repartition based on the account_id to optimize future joins \n",
    "trans_df = trans_df.withColumn(\"account_id\", col(\"account_id\").cast(\"int\")) \\\n",
    "                   .repartition(100, \"account_id\") \n",
    "account_df = account_df.withColumn(\"account_id\", col(\"account_id\").cast(\"int\"))\n",
    "\n",
    "\n",
    "# Cache the DataFrames to improve performance\n",
    "trans_df = trans_df.cache()\n",
    "account_df = account_df.cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Show the first few rows of both DataFrames for validation\n",
    "trans_df.show(2)\n",
    "account_df.show(2)\n",
    "\n",
    "\n",
    "# Count the number of rows in each DataFrame\n",
    "trans_count = trans_df.count()\n",
    "print(f\"Number of rows in 'trans_df': {trans_count}\")\n",
    "\n",
    "\n",
    "account_count = account_df.count()\n",
    "print(f\"Number of rows in 'account_df': {account_count}\")\n",
    "\n",
    "# Show the filtered transactions (valid account_id only)\n",
    "\n",
    "# Perform the join and broadcast the smaller account dataframe\n",
    "# valid_trans_df = trans_df.join(broadcast(account_df), \"account_id\", \"inner\")\n",
    "# Perform optimized broadcast join\n",
    "valid_trans_df = trans_df.join(broadcast(account_df), \"account_id\", \"inner\") \\\n",
    "                         .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Show sample only (avoid full count unless required)\n",
    "print(\"Sample valid transactions:\")\n",
    "valid_trans_df.show(5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "valid_trans_count_df = valid_trans_df.count()\n",
    "print(f\"Number of rows in 'valid_trans_df': {valid_trans_count_df}\")\n",
    "valid_trans_df.show(2)\n",
    "\n",
    "# Persist the DataFrame with valid transactions\n",
    "valid_trans_df = valid_trans_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "# Coalesce the DataFrame before saving to reduce the number of partitions\n",
    "valid_trans_df_coalesced = valid_trans_df.coalesce(1)\n",
    "\n",
    "\n",
    "# Show the filtered invalid transactions\n",
    "invalid_trans_df = trans_df.join(account_df, on=\"account_id\", how=\"left_anti\")\n",
    "print(f\"Number of transactions with invalid account_id: {invalid_trans_df.count()}\")\n",
    "invalid_trans_df.show(2)\n",
    "\n",
    "# OPTIONAL: Write to S3 (without coalesce for parallel write)\n",
    "# valid_trans_df.write \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .parquet(\"s3a://nmourmx-scigility/Silver/valid_trans_parquet/\")\n",
    "\n",
    "\n",
    "# Save the filtered DataFrame to S3 in the Silver folder as Parquet\n",
    "# valid_trans_df_coalesced.write \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .parquet(\"s3a://nmourmx-scigility/Silver/valid_trans_parquet/\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1109685-0bcd-4af1-9475-dd5a037d740b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample valid transactions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+-------+----------+------+--------------+-----------+\n",
      "|account_id|trans_id|amount|balance|      date|  type|     operation|district_id|\n",
      "+----------+--------+------+-------+----------+------+--------------+-----------+\n",
      "|      1645|  971490|  43.0|19460.0|1996-11-30|PRIJEM|          NULL|         50|\n",
      "|      3273|  589995|  15.0|26679.0|1997-04-30| VYDAJ|         VYBER|         74|\n",
      "|      1580|  285772|8400.0|41404.0|1998-04-12| VYDAJ|         VYBER|         69|\n",
      "|      2366|  425228| 388.0|24110.0|1998-02-07| VYDAJ|PREVOD NA UCET|         68|\n",
      "|      4519|  745408|2100.0|26200.0|1994-12-05|PRIJEM|         VKLAD|          8|\n",
      "+----------+--------+------+-------+----------+------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid transactions count: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+-------+----------+------+---------+\n",
      "|account_id|trans_id|amount|balance|      date|  type|operation|\n",
      "+----------+--------+------+-------+----------+------+---------+\n",
      "|     11500|  580601|4186.0|30123.0|1998-05-09| VYDAJ|    VYBER|\n",
      "|     11639|  858463|7753.0|22856.0|1994-11-07| VYDAJ|    VYBER|\n",
      "|     12139|  694873|4550.0| 9642.0|1998-08-22| VYDAJ|    VYBER|\n",
      "|     12100|  869665|9625.0|39800.0|1997-08-31| VYDAJ|    VYBER|\n",
      "|     11928|  939364|2148.0|73450.0|1993-02-28|PRIJEM|     NULL|\n",
      "+----------+--------+------+-------+----------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.functions import col, broadcast\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "if not aws_access_key_id or not aws_secret_access_key:\n",
    "    raise ValueError(\"AWS credentials are missing\")\n",
    "\n",
    "# Initialize Spark session with performance tuning\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Filter Invalid Transactions\") \\\n",
    "    .config(\"spark.master\", \"spark://spark-master-2:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key_id) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"62\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schemas\n",
    "trans_schema = StructType([\n",
    "    StructField(\"trans_id\", IntegerType(), True),\n",
    "    StructField(\"account_id\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"operation\", StringType(), True),\n",
    "    StructField(\"amount\", FloatType(), True),\n",
    "    StructField(\"balance\", FloatType(), True),\n",
    "    StructField(\"k_symbol\", StringType(), True),\n",
    "    StructField(\"branch\", StringType(), True),\n",
    "    StructField(\"bank\", StringType(), True),\n",
    "    StructField(\"account\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "account_schema = StructType([\n",
    "    StructField(\"account_id\", IntegerType(), True),\n",
    "    StructField(\"district_id\", IntegerType(), True),\n",
    "    StructField(\"frequency\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read only necessary columns from CSV\n",
    "trans_df = spark.read \\\n",
    "    .option(\"delimiter\", \";\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(trans_schema) \\\n",
    "    .csv(\"s3a://nmourmx-scigility/Bronze/trans/trans.csv\") \\\n",
    "    .select(\"trans_id\", \"account_id\", \"amount\", \"balance\", \"date\", \"type\", \"operation\")\n",
    "\n",
    "account_df = spark.read \\\n",
    "    .option(\"delimiter\", \";\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(account_schema) \\\n",
    "    .csv(\"s3a://nmourmx-scigility/Bronze/account/account.csv\") \\\n",
    "    .select(\"account_id\", \"district_id\")\n",
    "\n",
    "# Ensure both use the same data type\n",
    "trans_df = trans_df.withColumn(\"account_id\", col(\"account_id\").cast(\"int\")) \\\n",
    "                   .repartition(100, \"account_id\")  # Efficient shuffling\n",
    "account_df = account_df.withColumn(\"account_id\", col(\"account_id\").cast(\"int\"))\n",
    "\n",
    "# Perform optimized broadcast join\n",
    "valid_trans_df = trans_df.join(broadcast(account_df), \"account_id\", \"inner\") \\\n",
    "                         .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Show sample only (avoid full count unless required)\n",
    "print(\"Sample valid transactions:\")\n",
    "valid_trans_df.show(5)\n",
    "\n",
    "# Detect invalid transactions (those with account_id not in account_df)\n",
    "invalid_trans_df = trans_df.join(account_df, \"account_id\", \"left_anti\")\n",
    "\n",
    "print(f\"Invalid transactions count: {invalid_trans_df.count()}\")\n",
    "invalid_trans_df.show(5)\n",
    "\n",
    "# OPTIONAL: Write to S3 (without coalesce for parallel write)\n",
    "# valid_trans_df.write \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .parquet(\"s3a://nmourmx-scigility/Silver/valid_trans_parquet/\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab47b7-1d47-41fe-9b75-6c4d79746af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
