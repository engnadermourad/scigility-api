{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fadc0fe5-a07e-45d8-8180-5113269f30fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/01 20:43:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/01 20:43:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/01 20:43:38 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+------+--------------+-------+-------+--------+------+----+--------+\n",
      "|trans_id|account_id|      date|  type|     operation| amount|balance|k_symbol|branch|bank| account|\n",
      "+--------+----------+----------+------+--------------+-------+-------+--------+------+----+--------+\n",
      "|  967842|      1532|1998-05-31|PRIJEM|          NULL|  253.0|62661.0|    UROK|    AR|NULL|       0|\n",
      "|  271012|      1499|1998-01-09|PRIJEM|         VKLAD| 5500.0|49790.0|    NULL|    AR|NULL|       0|\n",
      "|  971490|      1645|1996-11-30|PRIJEM|          NULL|   43.0|19460.0|    UROK|    AR|NULL|       0|\n",
      "|  605699|      3366|1996-10-28| VYDAJ|         VYBER| 9300.0|38433.0|    NULL|    AR|NULL|       0|\n",
      "|  443107|      2471|1998-07-30| VYDAJ|         VYBER| 7800.0|56237.0|    NULL|    AR|NULL|       0|\n",
      "|  627433|      3503|1995-06-07| VYDAJ|         VYBER|28640.0|41949.0|    NULL|    AR|NULL|       0|\n",
      "|  390740|      2162|1997-12-03|PRIJEM|         VKLAD|11111.0|38640.0|    NULL|    AR|NULL|       0|\n",
      "|  516014|      2861|1997-01-13| VYDAJ|PREVOD NA UCET| 2613.0|25692.0|    SIPO|    AR|  GH|70139986|\n",
      "|  321504|      1782|1994-04-30| VYDAJ|         VYBER|  960.0|25793.0|    NULL|    AR|NULL|       0|\n",
      "|  329984|      1823|1997-02-05|PRIJEM| PREVOD Z UCTU| 4941.0|25705.0|  DUCHOD|    AR|  CD|91663467|\n",
      "|  230021|      1271|1996-12-31| VYDAJ|         VYBER|   15.0|18532.0|  SLUZBY|    AR|NULL|       0|\n",
      "|  347463|      1917|1997-06-01| VYDAJ|         VYBER| 3600.0|14446.0|    NULL|    AK|NULL|       0|\n",
      "|  684429|      3812|1996-03-05|PRIJEM| PREVOD Z UCTU| 4980.0|23308.0|  DUCHOD|    AR|  QR| 2473032|\n",
      "|  769731|      5591|1997-10-12| VYDAJ|PREVOD NA UCET|  345.0|47222.0|POJISTNE|    AR|  OP|87980109|\n",
      "|  322444|      1787|1996-09-13|PRIJEM|         VKLAD|35418.0|70470.0|    NULL|    AR|NULL|       0|\n",
      "|  515611|      2859|1998-08-17| VYBER|         VYBER|20117.0|43662.0|    NULL|    AR|NULL|       0|\n",
      "|  662943|      3696|1997-11-12| VYDAJ|PREVOD NA UCET| 3581.0|63201.0|    SIPO|    AR|  KL|77298909|\n",
      "|  993927|      2344|1998-10-31|PRIJEM|          NULL|  100.0|24423.0|    UROK|    AR|NULL|       0|\n",
      "|  690473|      3847|1996-05-06| VYDAJ|PREVOD NA UCET|    7.0|36798.0|    NULL|    AR|  YZ|83747803|\n",
      "|  488106|      2701|1997-05-05| VYDAJ|PREVOD NA UCET| 5761.0|50884.0|    NULL|    AR|  MN|24402356|\n",
      "+--------+----------+----------+------+--------------+-------+-------+--------+------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/01 20:44:17 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+------+--------------+-------+-------+--------+------+----+--------+\n",
      "|trans_id|account_id|      date|  type|     operation| amount|balance|k_symbol|branch|bank| account|\n",
      "+--------+----------+----------+------+--------------+-------+-------+--------+------+----+--------+\n",
      "|  967842|      1532|1998-05-31|PRIJEM|          NULL|  253.0|62661.0|    UROK|    AR|NULL|       0|\n",
      "|  271012|      1499|1998-01-09|PRIJEM|         VKLAD| 5500.0|49790.0|    NULL|    AR|NULL|       0|\n",
      "|  971490|      1645|1996-11-30|PRIJEM|          NULL|   43.0|19460.0|    UROK|    AR|NULL|       0|\n",
      "|  605699|      3366|1996-10-28| VYDAJ|         VYBER| 9300.0|38433.0|    NULL|    AR|NULL|       0|\n",
      "|  443107|      2471|1998-07-30| VYDAJ|         VYBER| 7800.0|56237.0|    NULL|    AR|NULL|       0|\n",
      "|  627433|      3503|1995-06-07| VYDAJ|         VYBER|28640.0|41949.0|    NULL|    AR|NULL|       0|\n",
      "|  390740|      2162|1997-12-03|PRIJEM|         VKLAD|11111.0|38640.0|    NULL|    AR|NULL|       0|\n",
      "|  516014|      2861|1997-01-13| VYDAJ|PREVOD NA UCET| 2613.0|25692.0|    SIPO|    AR|  GH|70139986|\n",
      "|  321504|      1782|1994-04-30| VYDAJ|         VYBER|  960.0|25793.0|    NULL|    AR|NULL|       0|\n",
      "|  329984|      1823|1997-02-05|PRIJEM| PREVOD Z UCTU| 4941.0|25705.0|  DUCHOD|    AR|  CD|91663467|\n",
      "|  230021|      1271|1996-12-31| VYDAJ|         VYBER|   15.0|18532.0|  SLUZBY|    AR|NULL|       0|\n",
      "|  347463|      1917|1997-06-01| VYDAJ|         VYBER| 3600.0|14446.0|    NULL|    AK|NULL|       0|\n",
      "|  684429|      3812|1996-03-05|PRIJEM| PREVOD Z UCTU| 4980.0|23308.0|  DUCHOD|    AR|  QR| 2473032|\n",
      "|  769731|      5591|1997-10-12| VYDAJ|PREVOD NA UCET|  345.0|47222.0|POJISTNE|    AR|  OP|87980109|\n",
      "|  322444|      1787|1996-09-13|PRIJEM|         VKLAD|35418.0|70470.0|    NULL|    AR|NULL|       0|\n",
      "|  515611|      2859|1998-08-17| VYBER|         VYBER|20117.0|43662.0|    NULL|    AR|NULL|       0|\n",
      "|  662943|      3696|1997-11-12| VYDAJ|PREVOD NA UCET| 3581.0|63201.0|    SIPO|    AR|  KL|77298909|\n",
      "|  993927|      2344|1998-10-31|PRIJEM|          NULL|  100.0|24423.0|    UROK|    AR|NULL|       0|\n",
      "|  690473|      3847|1996-05-06| VYDAJ|PREVOD NA UCET|    7.0|36798.0|    NULL|    AR|  YZ|83747803|\n",
      "|  488106|      2701|1997-05-05| VYDAJ|PREVOD NA UCET| 5761.0|50884.0|    NULL|    AR|  MN|24402356|\n",
      "+--------+----------+----------+------+--------------+-------+-------+--------+------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows in 'trans_df': 1056410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  type|\n",
      "+------+\n",
      "|PRIJEM|\n",
      "| VYBER|\n",
      "| VYDAJ|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get AWS credentials from environment variables\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"trans table data cleansing\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key_id) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .config(\"spark.master\", \"spark://spark-master-2:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"62\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the trans.csv file (match it to your CSV structure)\n",
    "schema = StructType([\n",
    "    StructField(\"trans_id\", IntegerType(), True),\n",
    "    StructField(\"account_id\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"operation\", StringType(), True),\n",
    "    StructField(\"amount\", FloatType(), True),\n",
    "    StructField(\"balance\", FloatType(), True),\n",
    "    StructField(\"k_symbol\", StringType(), True),\n",
    "    StructField(\"branch\", StringType(), True),\n",
    "    StructField(\"bank\", StringType(), True),\n",
    "    StructField(\"account\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Read the CSV file from the S3 bucket, use the first row as header\n",
    "df = spark.read \\\n",
    "    .option(\"delimiter\", \";\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"s3a://nmourmx-scigility/Bronze/trans/trans.csv\")  # Path to trans.csv inside the Bronze folder\n",
    "\n",
    "# Cache the DataFrame to improve performance\n",
    "df = df.cache()\n",
    "\n",
    "# Show the first few rows to verify the data\n",
    "df.show(20)\n",
    "\n",
    "# Persist the DataFrame with MEMORY_AND_DISK storage level (useful for large data)\n",
    "df = df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Replace the typo in the 'type' column: replace 'PRJIEM' with 'PRIJEM'\n",
    "df_fixed = df.withColumn(\"type\", \n",
    "                        when(df[\"type\"] == \"PRJIEM\", \"PRIJEM\").otherwise(df[\"type\"]))\n",
    "\n",
    "\n",
    "# Show the rows with the corrected type\n",
    "df_fixed.show(20)\n",
    "trans_count = df_fixed.count()\n",
    "# Print the count of rows in each DataFrame\n",
    "print(f\"Number of rows in 'trans_df': {trans_count}\")\n",
    "# Select distinct values from the 'type' column\n",
    "\n",
    "\n",
    "distinct_types = df_fixed.select(\"type\").distinct()\n",
    "\n",
    "# Show the distinct values\n",
    "distinct_types.show()\n",
    "\n",
    "# Save the fixed DataFrame to S3 in the Silver folder as Parquet\n",
    "df_fixed.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://nmourmx-scigility/Silver/trans_fixed_parquet/\")\n",
    "\n",
    "# # Save the distinct values of type to Parquet as well\n",
    "# distinct_types.write \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .parquet(\"s3a://nmourmx-scigility/Silver/trans_distinct_fixed_types/\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1109685-0bcd-4af1-9475-dd5a037d740b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
